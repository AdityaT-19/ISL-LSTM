{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change MediaPipe initialization to use Holistic instead of just Hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_finger_angles(landmarks):\n",
    "    \"\"\"Calculate bend angles for each finger\"\"\"\n",
    "    # Finger base joints indices\n",
    "    finger_bases = [1, 5, 9, 13, 17]  # Thumb, index, middle, ring, pinky bases\n",
    "    # Middle joints indices\n",
    "    middle_joints = [2, 6, 10, 14, 18]\n",
    "    # Tip joints indices\n",
    "    tip_joints = [4, 8, 12, 16, 20]\n",
    "    \n",
    "    angles = []\n",
    "    for base, mid, tip in zip(finger_bases, middle_joints, tip_joints):\n",
    "        # Get coordinates\n",
    "        base_coords = np.array([landmarks[base].x, landmarks[base].y])\n",
    "        mid_coords = np.array([landmarks[mid].x, landmarks[mid].y])\n",
    "        tip_coords = np.array([landmarks[tip].x, landmarks[tip].y])\n",
    "        \n",
    "        # Calculate vectors\n",
    "        v1 = mid_coords - base_coords\n",
    "        v2 = tip_coords - mid_coords\n",
    "        \n",
    "        # Calculate angle between vectors (in radians)\n",
    "        cosine_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "        \n",
    "        angles.append(angle)\n",
    "    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hand_to_hand_distances(left_landmarks, right_landmarks):\n",
    "    \"\"\"Calculate key distances between hands\"\"\"\n",
    "    # Key points to measure between hands\n",
    "    key_points = [0, 4, 8, 12, 20]  # Wrist, thumb tip, index tip, middle tip, pinky tip\n",
    "    \n",
    "    distances = []\n",
    "    for idx in key_points:\n",
    "        left_point = np.array([left_landmarks[idx].x, left_landmarks[idx].y])\n",
    "        right_point = np.array([right_landmarks[idx].x, right_landmarks[idx].y])\n",
    "        distance = np.linalg.norm(left_point - right_point)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_enhanced_keypoints(frame, holistic):\n",
    "    \"\"\"\n",
    "    Process the frame through MediaPipe Holistic to extract enhanced features including:\n",
    "    - Both hands landmarks\n",
    "    - Face landmarks (selected points)\n",
    "    - Hand motion features (velocity)\n",
    "    - Hand configuration metrics (angles)\n",
    "    Returns both the features array and the results object\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(frame_rgb)\n",
    "    \n",
    "    # Initialize empty feature array\n",
    "    features = []\n",
    "    \n",
    "    # 1. Extract left hand landmarks if detected\n",
    "    left_hand = []\n",
    "    if results.left_hand_landmarks:\n",
    "        for lm in results.left_hand_landmarks.landmark:\n",
    "            left_hand.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        # Fill with zeros if hand not detected\n",
    "        left_hand = [0] * (21 * 3)\n",
    "        \n",
    "    # 2. Extract right hand landmarks if detected\n",
    "    right_hand = []\n",
    "    if results.right_hand_landmarks:\n",
    "        for lm in results.right_hand_landmarks.landmark:\n",
    "            right_hand.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        # Fill with zeros if hand not detected\n",
    "        right_hand = [0] * (21 * 3)\n",
    "    \n",
    "    pose_landmarks = []\n",
    "    if results.pose_landmarks:\n",
    "        # 3. Extract pose landmarks if detected\n",
    "        for lm in results.pose_landmarks.landmark:\n",
    "            pose_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "    else:\n",
    "        # Fill with zeros if pose not detected\n",
    "        pose_landmarks = [0] * (33 * 3)\n",
    "    \n",
    "    # Combine all features\n",
    "    features.extend(left_hand)\n",
    "    features.extend(right_hand)\n",
    "    features.extend(pose_landmarks)\n",
    "    \n",
    "    # Add finger bend angles for left hand if detected\n",
    "    if results.left_hand_landmarks:\n",
    "        angles = calculate_finger_angles(results.left_hand_landmarks.landmark)\n",
    "        features.extend(angles)\n",
    "    else:\n",
    "        features.extend([0] * 5)  # One angle per finger\n",
    "        \n",
    "    # Add finger bend angles for right hand if detected\n",
    "    if results.right_hand_landmarks:\n",
    "        angles = calculate_finger_angles(results.right_hand_landmarks.landmark)\n",
    "        features.extend(angles)\n",
    "    else:\n",
    "        features.extend([0] * 5)  # One angle per finger\n",
    "        \n",
    "    # Calculate hand-to-hand distances if both hands detected\n",
    "    if results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "        distances = calculate_hand_to_hand_distances(\n",
    "            results.left_hand_landmarks.landmark,\n",
    "            results.right_hand_landmarks.landmark\n",
    "        )\n",
    "        features.extend(distances)\n",
    "    else:\n",
    "        features.extend([0] * 5)  # Key distances between hands\n",
    "        \n",
    "    return np.array(features), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_record_gesture(gesture_label, num_samples=30, sequence_length=30, \n",
    "                 output_dir=\"data/raw\", video_dir=\"data/videos\"):\n",
    "    \"\"\"\n",
    "    Enhanced version that captures richer features for ISL gestures\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    holistic = mp_holistic.Holistic(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    sample_count = 0\n",
    "    print(f\"Starting data collection for gesture: '{gesture_label}'\")\n",
    "    print(\"Press 's' to start recording a sample, or 'q' to quit.\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    \n",
    "    while sample_count < num_samples:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        \n",
    "        # Display instructions on the frame\n",
    "        cv2.putText(frame, f\"Gesture: {gesture_label} | Sample {sample_count+1}/{num_samples}\",\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Data Collection\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            print(\"Recording sample...\")\n",
    "            sequence = []\n",
    "            video_filename = os.path.join(video_dir, f\"{gesture_label}_{sample_count+1}.avi\")\n",
    "            height, width, _ = frame.shape\n",
    "            out = cv2.VideoWriter(video_filename, fourcc, 20.0, (width, height))\n",
    "            \n",
    "            prev_keypoints = None\n",
    "            \n",
    "            while len(sequence) < sequence_length:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                out.write(frame)  # Save frame to video file\n",
    "                \n",
    "                # Extract enhanced keypoints and get results\n",
    "                keypoints, results = extract_enhanced_keypoints(frame, holistic)\n",
    "                \n",
    "                # Calculate velocity if we have previous keypoints\n",
    "                if prev_keypoints is not None:\n",
    "                    velocity = keypoints - prev_keypoints\n",
    "                    # Append velocity features to current keypoints\n",
    "                    keypoints = np.concatenate([keypoints, velocity])\n",
    "                else:\n",
    "                    # For the first frame, use zeros for velocity\n",
    "                    velocity = np.zeros_like(keypoints)\n",
    "                    keypoints = np.concatenate([keypoints, velocity])\n",
    "                \n",
    "                # Save current keypoints for next frame velocity calculation\n",
    "                prev_keypoints = keypoints[:len(keypoints)//2]  # Only store original features, not velocity\n",
    "                \n",
    "                # Add to sequence\n",
    "                if keypoints is not None:\n",
    "                    sequence.append(keypoints)\n",
    "                \n",
    "                # Display frame with landmarks\n",
    "                if results.right_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                if results.left_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "               \n",
    "                \n",
    "                cv2.putText(frame, f\"Recording... {len(sequence)}/{sequence_length}\",\n",
    "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.imshow(\"Data Collection\", frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            \n",
    "            out.release()  # Finish saving the video\n",
    "            \n",
    "            if len(sequence) == sequence_length:\n",
    "                sequence = np.array(sequence)\n",
    "                filename = os.path.join(output_dir, f\"{gesture_label}_{sample_count+1}.npy\")\n",
    "                np.save(filename, sequence)\n",
    "                print(f\"Saved sample {sample_count+1} as {filename}\")\n",
    "                print(f\"Saved raw video as {video_filename}\")\n",
    "                sample_count += 1\n",
    "            else:\n",
    "                print(\"Incomplete sample, discarding...\")\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    holistic.close()\n",
    "    print(\"Data collection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = \"../data\"\n",
    "RAW_DATA_DIR = os.path.join(BASE_DATA_DIR, \"raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DATA_DIR, \"processed\")\n",
    "VIDEO_DIR = os.path.join(BASE_DATA_DIR, \"videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_video(video_path, output_dir):\n",
    "    \"\"\"\n",
    "    Extract keypoints from a video file and save them as numpy arrays\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    holistic = mp_holistic.Holistic(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    frame_count = 0\n",
    "    sequence = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process the frame\n",
    "        keypoints, results = extract_enhanced_keypoints(frame, holistic)\n",
    "        sequence.append(keypoints)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    # cv2.destroyAllWindows()\n",
    "    \n",
    "    # Save the extracted keypoints\n",
    "    sequence = np.array(sequence)\n",
    "    output_filename = os.path.join(output_dir, os.path.basename(video_path).replace('.avi', '.npy'))\n",
    "    np.save(output_filename, sequence)\n",
    "    \n",
    "    print(f\"Extracted keypoints saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keypoints saved to ../data\\raw\\bye_1.npy\n",
      "Extracted keypoints saved to ../data\\raw\\bye_10.npy\n",
      "Extracted keypoints saved to ../data\\raw\\bye_11.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m video_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      3\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(VIDEO_DIR, video_file)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mextract_keypoints_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRAW_DATA_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mextract_keypoints_from_video\u001b[1;34m(video_path, output_dir)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Process the frame\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m keypoints, results \u001b[38;5;241m=\u001b[39m \u001b[43mextract_enhanced_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholistic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m sequence\u001b[38;5;241m.\u001b[39mappend(keypoints)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mextract_enhanced_keypoints\u001b[1;34m(frame, holistic)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mProcess the frame through MediaPipe Holistic to extract enhanced features including:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m- Both hands landmarks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mReturns both the features array and the results object\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mholistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize empty feature array\u001b[39;00m\n\u001b[0;32m     14\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\beana\\OneDrive - 123\\Desktop\\Final-Year-Project\\lstm-pyt\\.venv\\Lib\\site-packages\\mediapipe\\python\\solutions\\holistic.py:160\u001b[0m, in \u001b[0;36mHolistic.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\beana\\OneDrive - 123\\Desktop\\Final-Year-Project\\lstm-pyt\\.venv\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:330\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    328\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m RGB_CHANNELS:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput image must contain three channel rgb data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 330\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_packet_to_input_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpacket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_stream_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulated_timestamp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m       packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(VIDEO_DIR):\n",
    "    if video_file.endswith('.avi'):\n",
    "        video_path = os.path.join(VIDEO_DIR, video_file)\n",
    "        extract_keypoints_from_video(video_path, output_dir=RAW_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels, sequence_length=30):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = []\n",
    "        self.label_map = {}\n",
    "        \n",
    "        # Load all gesture files\n",
    "        for label in labels:\n",
    "            files = glob.glob(os.path.join(data_dir, f\"{label}_*.npy\"))\n",
    "            for file in files:\n",
    "                keypoints = np.load(file)\n",
    "                if keypoints.shape[0] == sequence_length:\n",
    "                    self.data.append((keypoints, label))\n",
    "                    if label not in self.label_map:\n",
    "                        self.label_map[label] = len(self.label_map)  # Assign a unique index to each label\n",
    "        \n",
    "        random.shuffle(self.data)  # Shuffle the dataset for better training\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        keypoints, label = self.data[idx]\n",
    "        return torch.tensor(keypoints, dtype=torch.float32), torch.tensor(self.label_map[label], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Load the dataset and split into training and testing sets\n",
    "    \"\"\"\n",
    "    labels = [d.split('_')[0] for d in os.listdir(data_dir) if d.endswith('.npy')]\n",
    "    dataset = GestureDataset(data_dir, labels, sequence_length)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_data, test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create DataLoader for training and testing\n",
    "    train_dataset = GestureDataset(data_dir, [d[1] for d in train_data], sequence_length)\n",
    "    test_dataset = GestureDataset(data_dir, [d[1] for d in test_data], sequence_length)\n",
    "    \n",
    "    return train_dataset, test_dataset, dataset.label_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureRecognitionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(GestureRecognitionModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, 64,5 ,batch_first=True)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    data_dir, model_save_path, sequence_length=30, num_epochs=50, batch_size=32\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the gesture recognition model\n",
    "    \"\"\"\n",
    "    train_dataset, test_dataset, label_map = load_data(data_dir, sequence_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = train_dataset[0][0].shape[1]  # Number of features per frame\n",
    "    num_classes = len(label_map)  # Number of unique gestures\n",
    "\n",
    "    model = GestureRecognitionModel(input_size, num_classes)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    best_accuracy = 0.0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for keypoints, labels in train_loader:\n",
    "            keypoints, labels = keypoints.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(keypoints)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for keypoints, labels in test_loader:\n",
    "                keypoints, labels = keypoints.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(keypoints)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    print(\"Training complete. Best validation accuracy: {:.4f}\".format(best_accuracy))\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_loader, label_map):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set and print classification report\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for keypoints, labels in test_loader:\n",
    "            keypoints, labels = keypoints.to(device), labels.to(device)\n",
    "            outputs = model(keypoints)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    # Convert label map to a list of labels\n",
    "    label_list = list(label_map.keys())\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds, target_names=label_list))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_list, yticklabels=label_list)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history) + 1)\n",
    "    \n",
    "    train_loss = [h[0] for h in history]\n",
    "    train_acc = [h[1] for h in history]\n",
    "    val_loss = [h[2] for h in history]\n",
    "    val_acc = [h[3] for h in history]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_save_path):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(\"../models\", \"noface.v1.pth\")\n",
    "LOG_DIR = os.path.join(\"..\", \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model, history = train_model(RAW_DATA_DIR, MODEL_PATH, sequence_length=30, num_epochs=50, batch_size=32)\n",
    "plot_loss_accuracy(history)\n",
    "test_dataset, _, label_map = load_data(RAW_DATA_DIR, sequence_length=30)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "evaluate_model(model, test_loader, label_map)\n",
    "save_model(model, MODEL_PATH)\n",
    "# Save the label map to a file\n",
    "label_map_path = os.path.join(LOG_DIR, \"label_map.pkl\")\n",
    "with open(label_map_path, 'wb') as f:\n",
    "    pickle.dump(label_map, f)\n",
    "    \n",
    "# Save training history to a CSV file\n",
    "history_path = os.path.join(LOG_DIR, \"training_history.csv\")\n",
    "with open(history_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy', 'Validation Loss', 'Validation Accuracy'])\n",
    "    for epoch, (train_loss, train_acc, val_loss, val_acc) in enumerate(history):\n",
    "        writer.writerow([epoch + 1, train_loss, train_acc, val_loss, val_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_save_path, data_dir, sequence_length=30, batch_size=32):\n",
    "    \"\"\"\n",
    "    Load the trained model from a file\n",
    "    \"\"\"\n",
    "    train_dataset, _, label_map = load_data(data_dir, sequence_length)\n",
    "    input_size = train_dataset[0][0].shape[1]  # Number of features per frame\n",
    "    num_classes = len(label_map)  # Number of unique gestures\n",
    "\n",
    "    model = GestureRecognitionModel(input_size, num_classes)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_save_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_map(label_map_path):\n",
    "    \"\"\"\n",
    "    Load the label map from a file\n",
    "    \"\"\"\n",
    "    with open(label_map_path, 'rb') as f:\n",
    "        label_map = pickle.load(f)\n",
    "    print(f\"Label map loaded from {label_map_path}\")\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_prediction():\n",
    "    \"\"\"\n",
    "    Real-time gesture recognition using the trained model\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    model = load_model(MODEL_PATH, RAW_DATA_DIR)\n",
    "    label_map = load_label_map(os.path.join(LOG_DIR, \"label_map.pkl\"))\n",
    "    \n",
    "    holistic = mp_holistic.Holistic(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    sequence = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        keypoints, results = extract_enhanced_keypoints(frame, holistic)\n",
    "        \n",
    "        # Append keypoints to sequence\n",
    "        sequence.append(keypoints)\n",
    "        \n",
    "        # If we have enough frames in the sequence, make a prediction\n",
    "        if len(sequence) == 30:\n",
    "            sequence = np.array(sequence)\n",
    "            sequence = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(sequence)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                predicted_label = list(label_map.keys())[predicted.item()]\n",
    "            \n",
    "            # Display the predicted label on the frame\n",
    "            cv2.putText(frame, f\"Predicted: {predicted_label}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Clear the sequence for the next prediction\n",
    "            sequence = []\n",
    "        \n",
    "        # Draw landmarks on the frame\n",
    "        if results.right_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "        \n",
    "        cv2.imshow(\"Real-Time Gesture Recognition\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    holistic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_time_prediction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
