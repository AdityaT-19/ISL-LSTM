{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change MediaPipe initialization to use Holistic instead of just Hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(frame, hands):\n",
    "    \"\"\"\n",
    "    Process the frame through MediaPipe to extract hand landmarks.\n",
    "    Returns a flattened NumPy array of keypoints (x, y, z for each landmark) or None.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        landmarks = results.multi_hand_landmarks[0]\n",
    "        keypoints = []\n",
    "        for lm in landmarks.landmark:\n",
    "            keypoints.extend([lm.x, lm.y, lm.z])\n",
    "        return np.array(keypoints)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_jitter(sequence, noise_factor=0.02):\n",
    "    \"\"\"Add Gaussian noise to the keypoint sequence.\"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, sequence.shape)\n",
    "    return sequence + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_horizontal_flip(sequence):\n",
    "    \"\"\"Flip keypoints horizontally. Assumes x-coordinates are in [0, 1].\"\"\"\n",
    "    flipped = sequence.copy()\n",
    "    # Flip x-coordinate: 1 - x for each landmark (every 3 values starting at index 0)\n",
    "    for i in range(0, sequence.shape[1], 3):\n",
    "        flipped[:, i] = 1 - flipped[:, i]\n",
    "    return flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_random_rotation(sequence, max_angle=10):\n",
    "    \"\"\"Rotate keypoints around the center by a random small angle in degrees.\"\"\"\n",
    "    angle = np.deg2rad(random.uniform(-max_angle, max_angle))\n",
    "    cos_val = np.cos(angle)\n",
    "    sin_val = np.sin(angle)\n",
    "    rotated = sequence.copy()\n",
    "    # For each frame, rotate x and y for every landmark\n",
    "    for t in range(sequence.shape[0]):\n",
    "        for i in range(0, sequence.shape[1], 3):\n",
    "            x, y = sequence[t, i], sequence[t, i+1]\n",
    "            # Rotate around the center (0.5, 0.5) assuming normalized coords\n",
    "            x_centered, y_centered = x - 0.5, y - 0.5\n",
    "            x_new = x_centered * cos_val - y_centered * sin_val + 0.5\n",
    "            y_new = x_centered * sin_val + y_centered * cos_val + 0.5\n",
    "            rotated[t, i] = x_new\n",
    "            rotated[t, i+1] = y_new\n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_temporal_warp(sequence, min_stride=1, max_stride=2):\n",
    "    \"\"\"\n",
    "    Randomly sample the sequence with a stride between min_stride and max_stride.\n",
    "    This simulates temporal speed variations.\n",
    "    \"\"\"\n",
    "    stride = random.randint(min_stride, max_stride)\n",
    "    warped = sequence[::stride]\n",
    "    # If the warped sequence is too short, pad it with the last frame\n",
    "    while warped.shape[0] < sequence.shape[0]:\n",
    "        warped = np.vstack([warped, warped[-1]])\n",
    "    # Or truncate if too long\n",
    "    return warped[:sequence.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sequence(sequence, num_augments=5):\n",
    "    \"\"\"\n",
    "    Apply a combination of augmentation techniques to a keypoint sequence.\n",
    "    Returns a list of augmented sequences.\n",
    "    \"\"\"\n",
    "    augmented_sequences = []\n",
    "    for _ in range(num_augments):\n",
    "        aug_seq = sequence.copy()\n",
    "        # Randomly decide which augmentations to apply\n",
    "        if random.choice([True, False]):\n",
    "            aug_seq = augment_jitter(aug_seq, noise_factor=0.02)\n",
    "        if random.choice([True, False]):\n",
    "            aug_seq = augment_horizontal_flip(aug_seq)\n",
    "        if random.choice([True, False]):\n",
    "            aug_seq = augment_random_rotation(aug_seq, max_angle=10)\n",
    "        if random.choice([True, False]):\n",
    "            aug_seq = augment_temporal_warp(aug_seq, min_stride=1, max_stride=2)\n",
    "        augmented_sequences.append(aug_seq)\n",
    "    return augmented_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_gesture(gesture_label, num_samples=30, sequence_length=30, \n",
    "                   output_dir=\"data/raw\", video_dir=\"videos\"):\n",
    "    \"\"\"\n",
    "    Capture keypoint sequences for a given gesture.\n",
    "    - gesture_label: string label for the gesture (e.g., 'hello')\n",
    "    - num_samples: number of samples to record (default: 30)\n",
    "    - sequence_length: number of frames per sample sequence\n",
    "    - output_dir: directory to save the keypoint sequences\n",
    "    - video_dir: directory to save the raw video recordings\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=False,\n",
    "                           max_num_hands=1,\n",
    "                           min_detection_confidence=0.7)\n",
    "    \n",
    "    sample_count = 0\n",
    "    print(f\"Starting data collection for gesture: '{gesture_label}'\")\n",
    "    print(\"Press 's' to start recording a sample, or 'q' to quit.\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    \n",
    "    while sample_count < num_samples:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        \n",
    "        # Display instructions on the frame\n",
    "        cv2.putText(frame, f\"Gesture: {gesture_label} | Sample {sample_count+1}/{num_samples}\",\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Data Collection\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('s'):\n",
    "            print(\"Recording sample...\")\n",
    "            sequence = []\n",
    "            # Setup video writer to save the raw video\n",
    "            video_filename = os.path.join(video_dir, f\"{gesture_label}_{sample_count+1}.avi\")\n",
    "            height, width, _ = frame.shape\n",
    "            out = cv2.VideoWriter(video_filename, fourcc, 20.0, (width, height))\n",
    "            \n",
    "            while len(sequence) < sequence_length:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                out.write(frame)  # Save frame to video file\n",
    "                keypoints = extract_keypoints(frame, hands)\n",
    "                # Only add frames where keypoints are detected\n",
    "                if keypoints is not None:\n",
    "                    sequence.append(keypoints)\n",
    "                cv2.putText(frame, f\"Recording... {len(sequence)}/{sequence_length}\",\n",
    "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.imshow(\"Data Collection\", frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            out.release()  # Finish saving the video\n",
    "            \n",
    "            if len(sequence) == sequence_length:\n",
    "                sequence = np.array(sequence)\n",
    "                filename = os.path.join(output_dir, f\"{gesture_label}_{sample_count+1}.npy\")\n",
    "                np.save(filename, sequence)\n",
    "                print(f\"Saved sample {sample_count+1} as {filename}\")\n",
    "                print(f\"Saved raw video as {video_filename}\")\n",
    "                sample_count += 1\n",
    "            else:\n",
    "                print(\"Incomplete sample, discarding...\")\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    hands.close()\n",
    "    print(\"Data collection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(input_dir=\"data/raw\", output_dir=\"data/processed\", num_augments=5):\n",
    "    \"\"\"\n",
    "    Process all .npy files in the input directory, apply multiple augmentation techniques,\n",
    "    and save the augmented sequences in the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".npy\"):\n",
    "            filepath = os.path.join(input_dir, file)\n",
    "            sequence = np.load(filepath)\n",
    "            augmented_sequences = augment_sequence(sequence, num_augments=num_augments)\n",
    "            for idx, aug_seq in enumerate(augmented_sequences):\n",
    "                new_filename = os.path.join(output_dir, f\"{file.split('.')[0]}_aug{idx+1}.npy\")\n",
    "                np.save(new_filename, aug_seq)\n",
    "                print(f\"Saved augmented data: {new_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = \"../data\"\n",
    "RAW_DATA_DIR = os.path.join(BASE_DATA_DIR, \"raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DATA_DIR, \"processed\")\n",
    "VIDEO_DIR = os.path.join(BASE_DATA_DIR, \"videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will record 30 samples for this gesture.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'VideoCapture'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m gesture_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter gesture label (e.g., hello, bye, thankyou): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe will record 30 samples for this gesture.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrecord_gesture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgesture_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRAW_DATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVIDEO_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m augment_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo you want to augment the data? (y/n): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment_choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m, in \u001b[0;36mrecord_gesture\u001b[1;34m(gesture_label, num_samples, sequence_length, output_dir, video_dir)\u001b[0m\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(video_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 14\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     15\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n\u001b[0;32m     16\u001b[0m hands \u001b[38;5;241m=\u001b[39m mp_hands\u001b[38;5;241m.\u001b[39mHands(static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m                        max_num_hands\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     18\u001b[0m                        min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'VideoCapture'"
     ]
    }
   ],
   "source": [
    "gesture_label = input(\"Enter gesture label (e.g., hello, bye, thankyou): \").strip()\n",
    "print(\"We will record 30 samples for this gesture.\")\n",
    "record_gesture(gesture_label, num_samples=30, output_dir=RAW_DATA_DIR, video_dir=VIDEO_DIR)\n",
    "\n",
    "augment_choice = input(\"Do you want to augment the data? (y/n): \").strip().lower()\n",
    "if augment_choice == 'y':\n",
    "    augment_data(input_dir=RAW_DATA_DIR, output_dir=PROCESSED_DATA_DIR, num_augments=5)\n",
    "    print(\"Data augmentation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
