{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    Extract and combine keypoints from MediaPipe Holistic results.\n",
    "    \n",
    "    Returns a flattened array containing:\n",
    "    1. Left hand landmarks (if detected)\n",
    "    2. Right hand landmarks (if detected)\n",
    "    3. Pose landmarks (upper body only)\n",
    "    4. Calculated relative positions and angles\n",
    "    \"\"\"\n",
    "    # Extract left hand landmarks (21 landmarks x 3 coordinates = 63 values)\n",
    "    lh = np.zeros(63) if results.left_hand_landmarks is None else \\\n",
    "         np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]).flatten()\n",
    "    \n",
    "    # Extract right hand landmarks (21 landmarks x 3 coordinates = 63 values)\n",
    "    rh = np.zeros(63) if results.right_hand_landmarks is None else \\\n",
    "         np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]).flatten()\n",
    "    \n",
    "    # Extract pose landmarks (upper body only - 11 landmarks x 3 coordinates = 33 values)\n",
    "    # We use landmarks for shoulders, elbows, wrists, hips, and neck\n",
    "    upper_body_indices = [0, 11, 12, 13, 14, 15, 16, 23, 24]  # Key upper body landmarks\n",
    "    pose = np.zeros(len(upper_body_indices) * 3)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        for i, idx in enumerate(upper_body_indices):\n",
    "            if idx < len(landmarks):\n",
    "                pose[i*3:(i*3)+3] = [landmarks[idx].x, landmarks[idx].y, landmarks[idx].z]\n",
    "    \n",
    "    # Calculate relative positions (hands to shoulders)\n",
    "    relative_features = []\n",
    "    \n",
    "    if results.pose_landmarks and (results.left_hand_landmarks or results.right_hand_landmarks):\n",
    "        # Reference points (shoulders)\n",
    "        left_shoulder = np.array([landmarks[11].x, landmarks[11].y, landmarks[11].z]) if 11 < len(landmarks) else np.zeros(3)\n",
    "        right_shoulder = np.array([landmarks[12].x, landmarks[12].y, landmarks[12].z]) if 12 < len(landmarks) else np.zeros(3)\n",
    "        \n",
    "        # Hand center points (if detected)\n",
    "        if results.left_hand_landmarks:\n",
    "            left_hand_center = np.mean(np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]), axis=0)\n",
    "            # Distance from left hand to left shoulder\n",
    "            relative_features.extend(left_hand_center - left_shoulder)\n",
    "        else:\n",
    "            relative_features.extend(np.zeros(3))\n",
    "        \n",
    "        if results.right_hand_landmarks:\n",
    "            right_hand_center = np.mean(np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]), axis=0)\n",
    "            # Distance from right hand to right shoulder\n",
    "            relative_features.extend(right_hand_center - right_shoulder)\n",
    "        else:\n",
    "            relative_features.extend(np.zeros(3))\n",
    "            \n",
    "        # Distance between hands (if both detected)\n",
    "        if results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "            hand_distance = right_hand_center - left_hand_center\n",
    "            relative_features.extend(hand_distance)\n",
    "        else:\n",
    "            relative_features.extend(np.zeros(3))\n",
    "    else:\n",
    "        # Add placeholder zeros if landmarks aren't detected\n",
    "        relative_features = np.zeros(9)  # 3 sets of x,y,z coordinates\n",
    "    \n",
    "    # Combine all features\n",
    "    return np.concatenate([lh, rh, pose, relative_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity(current_frame, previous_frame, time_delta=1.0):\n",
    "    \"\"\"Calculate velocity features between two frames of landmarks\"\"\"\n",
    "    if previous_frame is None:\n",
    "        return np.zeros(len(current_frame))\n",
    "    \n",
    "    # Simple velocity calculation (displacement / time)\n",
    "    return (current_frame - previous_frame) / time_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trajectory_features(sequence, landmark_indices=[0, 9, 13, 17]):\n",
    "    \"\"\"\n",
    "    Extract trajectory-based features for specific landmarks across frames.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Array of shape (frames, features) containing landmark positions\n",
    "        landmark_indices: Indices of key landmarks to track (e.g., wrist, fingertips)\n",
    "    \n",
    "    Returns:\n",
    "        Trajectory features including direction changes, curvature, and path length\n",
    "    \"\"\"\n",
    "    trajectory_features = []\n",
    "    \n",
    "    # For each key landmark\n",
    "    for idx in landmark_indices:\n",
    "        base_idx = idx * 3  # Each landmark has x,y,z\n",
    "        \n",
    "        # Extract trajectory of this landmark across all frames\n",
    "        x_pos = sequence[:, base_idx]\n",
    "        y_pos = sequence[:, base_idx + 1]\n",
    "        z_pos = sequence[:, base_idx + 2]\n",
    "        \n",
    "        # Calculate path length (cumulative distance traveled)\n",
    "        dx = np.diff(x_pos)\n",
    "        dy = np.diff(y_pos)\n",
    "        dz = np.diff(z_pos)\n",
    "        path_length = np.sum(np.sqrt(dx**2 + dy**2 + dz**2))\n",
    "        \n",
    "        # Count direction changes (when velocity changes sign)\n",
    "        direction_changes_x = np.sum(np.abs(np.diff(np.sign(dx))))\n",
    "        direction_changes_y = np.sum(np.abs(np.diff(np.sign(dy))))\n",
    "        direction_changes = direction_changes_x + direction_changes_y\n",
    "        \n",
    "        # Calculate average curvature (needs at least 3 points)\n",
    "        curvature = 0\n",
    "        if len(x_pos) > 2:\n",
    "            # Approximate curvature using finite differences\n",
    "            dx_dt = np.gradient(x_pos)\n",
    "            dy_dt = np.gradient(y_pos)\n",
    "            d2x_dt2 = np.gradient(dx_dt)\n",
    "            d2y_dt2 = np.gradient(dy_dt)\n",
    "            curvature = np.mean(np.abs(dx_dt*d2y_dt2 - dy_dt*d2x_dt2) / \n",
    "                               (dx_dt**2 + dy_dt**2)**(3/2))\n",
    "            \n",
    "        trajectory_features.extend([path_length, direction_changes, curvature])\n",
    "    \n",
    "    return np.array(trajectory_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dynamics_features(sequence):\n",
    "    \"\"\"\n",
    "    Calculate acceleration and jerk features to capture motion dynamics.\n",
    "    \"\"\"\n",
    "    # Get position data (first half of features if velocity is included)\n",
    "    positions = sequence[:, :sequence.shape[1]//2] if sequence.shape[1] > 200 else sequence\n",
    "    \n",
    "    # Calculate velocities (first derivative)\n",
    "    velocities = np.gradient(positions, axis=0)\n",
    "    \n",
    "    # Calculate accelerations (second derivative)\n",
    "    accelerations = np.gradient(velocities, axis=0)\n",
    "    \n",
    "    # Calculate jerk (third derivative)\n",
    "    jerk = np.gradient(accelerations, axis=0)\n",
    "    \n",
    "    # Compute statistical features from these signals\n",
    "    acc_features = []\n",
    "    \n",
    "    # For acceleration\n",
    "    acc_mean = np.mean(np.linalg.norm(accelerations, axis=1))\n",
    "    acc_std = np.std(np.linalg.norm(accelerations, axis=1))\n",
    "    acc_max = np.max(np.linalg.norm(accelerations, axis=1))\n",
    "    \n",
    "    # For jerk - higher derivatives capture suddenness of movements\n",
    "    jerk_mean = np.mean(np.linalg.norm(jerk, axis=1))\n",
    "    jerk_std = np.std(np.linalg.norm(jerk, axis=1))\n",
    "    jerk_max = np.max(np.linalg.norm(jerk, axis=1))\n",
    "    \n",
    "    # Combine features\n",
    "    acc_features = [acc_mean, acc_std, acc_max, jerk_mean, jerk_std, jerk_max]\n",
    "    \n",
    "    return np.array(acc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_shape_dynamics(results):\n",
    "    \"\"\"\n",
    "    Extract features related to hand shape changes over time.\n",
    "    \"\"\"\n",
    "    shape_features = []\n",
    "    \n",
    "    # If we have hand landmarks\n",
    "    if results.right_hand_landmarks:\n",
    "        landmarks = results.right_hand_landmarks.landmark\n",
    "        \n",
    "        # 1. Calculate finger spread (distance between fingertips)\n",
    "        fingertips = [4, 8, 12, 16, 20]  # Thumb, index, middle, ring, pinky\n",
    "        fingertip_positions = [np.array([landmarks[idx].x, landmarks[idx].y, landmarks[idx].z]) \n",
    "                              for idx in fingertips]\n",
    "        \n",
    "        # Measure spread: sum of distances between adjacent fingertips\n",
    "        finger_spread = 0\n",
    "        for i in range(len(fingertips)-1):\n",
    "            finger_spread += np.linalg.norm(fingertip_positions[i] - fingertip_positions[i+1])\n",
    "        \n",
    "        # 2. Calculate palm area (approximated by triangulation)\n",
    "        palm_points = [0, 1, 5, 9, 13, 17]  # Wrist and knuckles\n",
    "        palm_positions = [np.array([landmarks[idx].x, landmarks[idx].y]) \n",
    "                         for idx in palm_points]\n",
    "        # Approximate area using cross-product of vectors\n",
    "        palm_area = 0\n",
    "        for i in range(1, len(palm_points)-1):\n",
    "            v1 = palm_positions[i] - palm_positions[0]\n",
    "            v2 = palm_positions[i+1] - palm_positions[0]\n",
    "            palm_area += 0.5 * abs(np.cross(v1, v2))\n",
    "        \n",
    "        # 3. Thumb-index pinch distance (important for many signs)\n",
    "        thumb_tip = np.array([landmarks[4].x, landmarks[4].y, landmarks[4].z])\n",
    "        index_tip = np.array([landmarks[8].x, landmarks[8].y, landmarks[8].z])\n",
    "        pinch_distance = np.linalg.norm(thumb_tip - index_tip)\n",
    "        \n",
    "        shape_features = [finger_spread, palm_area, pinch_distance]\n",
    "    else:\n",
    "        shape_features = [0, 0, 0]  # Default values if no hand detected\n",
    "        \n",
    "    return np.array(shape_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequency_features(sequence, n_components=5):\n",
    "    \"\"\"\n",
    "    Extract frequency domain features using Fast Fourier Transform.\n",
    "    Useful for capturing repetitive patterns in sign movements.\n",
    "    \"\"\"\n",
    "    # Get only hand landmark positions\n",
    "    hand_landmarks = sequence[:, :126]  # Assuming first 126 values are hand landmarks\n",
    "    \n",
    "    # Initialize arrays for frequency features\n",
    "    fft_features = []\n",
    "    \n",
    "    # For computational efficiency, analyze only a subset of landmarks\n",
    "    key_landmarks = [0, 4, 8, 12, 16, 20]  # Wrist, thumb tip, and fingertips\n",
    "    \n",
    "    for lm in key_landmarks:\n",
    "        base_idx = lm * 3  # Each landmark has x,y,z\n",
    "        \n",
    "        # Extract movement for this landmark\n",
    "        x_pos = hand_landmarks[:, base_idx]\n",
    "        y_pos = hand_landmarks[:, base_idx + 1]\n",
    "        \n",
    "        # Apply FFT to get frequency components\n",
    "        fft_x = np.abs(np.fft.rfft(x_pos))\n",
    "        fft_y = np.abs(np.fft.rfft(y_pos))\n",
    "        \n",
    "        # Get dominant frequencies (largest magnitude components)\n",
    "        # This captures the main rhythmic elements of the movement\n",
    "        top_k_x = fft_x.argsort()[-n_components:][::-1]\n",
    "        top_k_y = fft_y.argsort()[-n_components:][::-1]\n",
    "        \n",
    "        # Frequency values\n",
    "        freq_x = top_k_x / len(x_pos)\n",
    "        freq_y = top_k_y / len(y_pos)\n",
    "        \n",
    "        # Amplitude values (normalized)\n",
    "        amp_x = fft_x[top_k_x] / np.max(fft_x) if np.max(fft_x) > 0 else fft_x[top_k_x]\n",
    "        amp_y = fft_y[top_k_y] / np.max(fft_y) if np.max(fft_y) > 0 else fft_y[top_k_y]\n",
    "        \n",
    "        # Add these frequency features\n",
    "        fft_features.extend(np.concatenate([freq_x, freq_y, amp_x, amp_y]))\n",
    "    \n",
    "    return np.array(fft_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relative_movement(sequence):\n",
    "    \"\"\"\n",
    "    Extract features that describe the relationship between left and right hand movements.\n",
    "    \"\"\"\n",
    "    # Separate left and right hand data\n",
    "    lh = sequence[:, :63]  # First 63 features are left hand\n",
    "    rh = sequence[:, 63:126]  # Next 63 features are right hand\n",
    "    \n",
    "    # Calculate hand centers (average position of all landmarks)\n",
    "    lh_center = np.mean(lh.reshape(lh.shape[0], -1, 3), axis=1)\n",
    "    rh_center = np.mean(rh.reshape(rh.shape[0], -1, 3), axis=1)\n",
    "    \n",
    "    # Calculate relative features\n",
    "    relative_features = []\n",
    "    \n",
    "    # 1. Movement correlation between hands\n",
    "    # High correlation means hands move together, negative means opposite movement\n",
    "    lh_vel = np.gradient(lh_center, axis=0)\n",
    "    rh_vel = np.gradient(rh_center, axis=0)\n",
    "    \n",
    "    # Correlation coefficient between left and right hand velocity\n",
    "    corr_x = np.corrcoef(lh_vel[:, 0], rh_vel[:, 0])[0, 1] if len(lh_vel) > 1 else 0\n",
    "    corr_y = np.corrcoef(lh_vel[:, 1], rh_vel[:, 1])[0, 1] if len(lh_vel) > 1 else 0\n",
    "    corr_z = np.corrcoef(lh_vel[:, 2], rh_vel[:, 2])[0, 1] if len(lh_vel) > 1 else 0\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    corr_x = 0 if np.isnan(corr_x) else corr_x\n",
    "    corr_y = 0 if np.isnan(corr_y) else corr_y\n",
    "    corr_z = 0 if np.isnan(corr_z) else corr_z\n",
    "    \n",
    "    # 2. Relative distance between hands over time\n",
    "    distances = np.linalg.norm(lh_center - rh_center, axis=1)\n",
    "    dist_mean = np.mean(distances)\n",
    "    dist_std = np.std(distances)\n",
    "    dist_change = np.max(distances) - np.min(distances)  # Total change in distance\n",
    "    \n",
    "    # 3. Relative angle between hands\n",
    "    angle_features = []\n",
    "    for i in range(len(lh_center)-1):\n",
    "        # Vector of movement for each hand\n",
    "        lh_move = lh_center[i+1] - lh_center[i]\n",
    "        rh_move = rh_center[i+1] - rh_center[i]\n",
    "        \n",
    "        # Calculate angle between vectors (if vectors have magnitude)\n",
    "        lh_mag = np.linalg.norm(lh_move)\n",
    "        rh_mag = np.linalg.norm(rh_move)\n",
    "        \n",
    "        if lh_mag > 0 and rh_mag > 0:\n",
    "            cos_angle = np.dot(lh_move, rh_move) / (lh_mag * rh_mag)\n",
    "            # Clip to handle numerical errors\n",
    "            cos_angle = np.clip(cos_angle, -1.0, 1.0)\n",
    "            angle = np.arccos(cos_angle)\n",
    "            angle_features.append(angle)\n",
    "    \n",
    "    # Average relative angle (in radians)\n",
    "    avg_angle = np.mean(angle_features) if angle_features else 0\n",
    "    \n",
    "    # Combine all relative movement features\n",
    "    relative_features = [corr_x, corr_y, corr_z, dist_mean, dist_std, dist_change, avg_angle]\n",
    "    \n",
    "    return np.array(relative_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(results, previous_results_buffer=None):\n",
    "    \"\"\"\n",
    "    Extract comprehensive features ensuring consistent dimensions.\n",
    "    \"\"\"\n",
    "    # Extract basic keypoints\n",
    "    keypoints = extract_keypoints(results)\n",
    "    \n",
    "    # If we don't have previous results, pad with zeros to match full feature size\n",
    "    if previous_results_buffer is None or len(previous_results_buffer) < 1:\n",
    "        # Create zero velocities and other features to maintain consistent dimensions\n",
    "        velocities = np.zeros_like(keypoints)\n",
    "        # Add any other features with appropriate dimensions\n",
    "        dynamic_features = np.zeros(30)  # Adjust this size to match your other dynamic features\n",
    "        \n",
    "        # Combine to ensure consistent dimensions with the full feature set\n",
    "        return np.concatenate([keypoints, velocities, dynamic_features])\n",
    "    \n",
    "    # If we have previous results, calculate all features\n",
    "    else:\n",
    "        # Get the most recent result for velocity calculation\n",
    "        previous_result = previous_results_buffer[-1]\n",
    "        previous_keypoints = extract_keypoints(previous_result)\n",
    "        \n",
    "        # Calculate velocity features\n",
    "        velocities = calculate_velocity(keypoints, previous_keypoints)\n",
    "        \n",
    "        # Calculate other dynamic features - ensure consistent dimensions\n",
    "        dynamic_features = np.zeros(30)  # Placeholder - replace with your actual features\n",
    "        \n",
    "        # Return combined features with consistent dimensions\n",
    "        return np.concatenate([keypoints, velocities, dynamic_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(input_dir=\"../data/raw\", output_dir=\"../data/processed\", num_augments=5):\n",
    "    \"\"\"\n",
    "    Process all .npy files in the input directory, apply multiple augmentation techniques,\n",
    "    and save the augmented sequences in the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".npy\"):\n",
    "            filepath = os.path.join(input_dir, file)\n",
    "            sequence = np.load(filepath)\n",
    "            \n",
    "            # Original sequence - also save to processed folder\n",
    "            original_filename = os.path.join(output_dir, file)\n",
    "            np.save(original_filename, sequence)\n",
    "            print(f\"Saved original sequence: {original_filename}\")\n",
    "            \n",
    "            # Generate augmented sequences\n",
    "            augmented_sequences = []\n",
    "            \n",
    "            # 1. Add noise (jitter)\n",
    "            for i in range(2):  # Two versions with different noise levels\n",
    "                noise_factor = 0.01 + (i * 0.01)  # 0.01 and 0.02\n",
    "                noisy_seq = sequence.copy()\n",
    "                # Only add noise to the landmark positions, not to velocity features\n",
    "                landmarks_size = sequence.shape[1] // 2  # First half is landmarks, second half is velocity\n",
    "                noise = np.random.normal(0, noise_factor, (sequence.shape[0], landmarks_size))\n",
    "                noisy_seq[:, :landmarks_size] += noise\n",
    "                augmented_sequences.append((noisy_seq, f\"noise{i+1}\"))\n",
    "            \n",
    "            # 2. Time warping (speed variation)\n",
    "            for i in range(2):\n",
    "                warp_factor = 0.8 + (i * 0.4)  # 0.8 (slower) and 1.2 (faster)\n",
    "                warped_indices = np.linspace(0, sequence.shape[0]-1, int(sequence.shape[0] * warp_factor))\n",
    "                warped_indices = np.clip(warped_indices, 0, sequence.shape[0]-1).astype(int)\n",
    "                warped_seq = sequence[warped_indices].copy()\n",
    "                \n",
    "                # Resize to original sequence length\n",
    "                if warped_seq.shape[0] != sequence.shape[0]:\n",
    "                    # Use linear interpolation to resize\n",
    "                    from scipy.interpolate import interp1d\n",
    "                    x_original = np.linspace(0, 1, warped_seq.shape[0])\n",
    "                    x_new = np.linspace(0, 1, sequence.shape[0])\n",
    "                    warped_resized = np.zeros_like(sequence)\n",
    "                    \n",
    "                    for j in range(warped_seq.shape[1]):\n",
    "                        interpolator = interp1d(x_original, warped_seq[:, j], kind='linear')\n",
    "                        warped_resized[:, j] = interpolator(x_new)\n",
    "                    \n",
    "                    warped_seq = warped_resized\n",
    "                \n",
    "                augmented_sequences.append((warped_seq, f\"warp{i+1}\"))\n",
    "            \n",
    "            # 3. Spatial translation (small shifts)\n",
    "            for i in range(1):\n",
    "                shift_factor = 0.05  # Small spatial shift\n",
    "                shifted_seq = sequence.copy()\n",
    "                # Only shift the position coordinates (x,y), not z or velocities\n",
    "                landmarks_size = sequence.shape[1] // 2\n",
    "                \n",
    "                # Create shift values for x and y coordinates\n",
    "                x_shift = np.random.uniform(-shift_factor, shift_factor)\n",
    "                y_shift = np.random.uniform(-shift_factor, shift_factor)\n",
    "                \n",
    "                # Apply shifts to every 3rd value starting at index 0 (x) and index 1 (y)\n",
    "                for j in range(0, landmarks_size, 3):\n",
    "                    shifted_seq[:, j] += x_shift     # shift x coordinates\n",
    "                    shifted_seq[:, j+1] += y_shift   # shift y coordinates\n",
    "                \n",
    "                augmented_sequences.append((shifted_seq, f\"shift{i+1}\"))\n",
    "            \n",
    "            # Save all augmented sequences\n",
    "            for aug_seq, aug_type in augmented_sequences:\n",
    "                base_filename = file.split('.')[0]\n",
    "                new_filename = os.path.join(output_dir, f\"{base_filename}_{aug_type}.npy\")\n",
    "                np.save(new_filename, aug_seq)\n",
    "                print(f\"Saved augmented data: {new_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_gesture(gesture_label, num_samples=30, sequence_length=30, \n",
    "                   output_dir=\"../data/raw\", video_dir=\"../data/videos\"):\n",
    "    \"\"\"\n",
    "    Improved capture of keypoint sequences for Indian Sign Language gestures.\n",
    "    Includes both hand and pose data, plus derived features.\n",
    "    \n",
    "    Arguments:\n",
    "    - gesture_label: string label for the gesture (e.g., 'namaste')\n",
    "    - num_samples: number of samples to record\n",
    "    - sequence_length: number of frames per sample sequence\n",
    "    - output_dir: directory to save the keypoint sequences\n",
    "    - video_dir: directory to save the raw video recordings\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    \n",
    "    # Open webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Initialize MediaPipe Holistic\n",
    "    with mp_holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        sample_count = 0\n",
    "        print(f\"Starting data collection for gesture: '{gesture_label}'\")\n",
    "        print(\"Press 's' to start recording a sample, or 'q' to quit.\")\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        countdown_active = False\n",
    "        countdown_timer = 0\n",
    "        \n",
    "        while sample_count < num_samples:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            \n",
    "            # Flip horizontally for a more intuitive mirror view\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Convert to RGB for MediaPipe\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process the frame for visualization only (not recording yet)\n",
    "            results = holistic.process(frame_rgb)\n",
    "            \n",
    "            # Draw landmarks for visual feedback\n",
    "            annotated_frame = frame.copy()\n",
    "            # Draw pose landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_frame,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "            \n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_frame,\n",
    "                results.left_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_frame,\n",
    "                results.right_hand_landmarks,\n",
    "                mp_holistic.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.get_default_hand_landmarks_style())\n",
    "            \n",
    "            # Display instructions on the frame\n",
    "            if countdown_active:\n",
    "                # Show countdown timer\n",
    "                remaining = int(countdown_timer - time.time())\n",
    "                if remaining > 0:\n",
    "                    cv2.putText(annotated_frame, f\"Starting in {remaining}...\", \n",
    "                                (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                else:\n",
    "                    # Start recording after countdown\n",
    "                    countdown_active = False\n",
    "                    print(\"Recording sample...\")\n",
    "                    sequence = []\n",
    "                    previous_results_buffer = []\n",
    "                    \n",
    "                    # Setup video writer to save the raw video\n",
    "                    video_filename = os.path.join(video_dir, f\"{gesture_label}_{sample_count+1}.avi\")\n",
    "                    height, width, _ = frame.shape\n",
    "                    out = cv2.VideoWriter(video_filename, fourcc, 20.0, (width, height))\n",
    "                    \n",
    "                    recording_start = time.time()\n",
    "                    \n",
    "                    # Recording loop\n",
    "                    while len(sequence) < sequence_length:\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            continue\n",
    "                            \n",
    "                        frame = cv2.flip(frame, 1)\n",
    "                        out.write(frame)  # Save frame to video file\n",
    "                        \n",
    "                        # Process frame with MediaPipe\n",
    "                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        results = holistic.process(frame_rgb)\n",
    "                        \n",
    "                        # Extract keypoints\n",
    "                        results = holistic.process(frame_rgb)\n",
    "                        if len(previous_results_buffer) >= 5:  # Need some history for dynamics\n",
    "                            features = extract_all_features(results, previous_results_buffer)\n",
    "                            sequence.append(features)\n",
    "                            # Update buffer (keep last 5 frames)\n",
    "                            previous_results_buffer.pop(0)\n",
    "                            previous_results_buffer.append(results)\n",
    "                        else:\n",
    "                            # Building initial buffer\n",
    "                            previous_results_buffer.append(results)\n",
    "                            features = extract_keypoints(results)  # Basic features only\n",
    "                            sequence.append(features)\n",
    "                        \n",
    "                        \n",
    "                        # Draw landmarks on the frame\n",
    "                        annotated_frame = frame.copy()\n",
    "                        mp_drawing.draw_landmarks(annotated_frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                        mp_drawing.draw_landmarks(annotated_frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                        mp_drawing.draw_landmarks(annotated_frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                        \n",
    "                        # Show recording progress\n",
    "                        elapsed = time.time() - recording_start\n",
    "                        cv2.putText(annotated_frame, f\"Recording... {len(sequence)}/{sequence_length}\",\n",
    "                                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        cv2.putText(annotated_frame, f\"Time: {elapsed:.1f}s\", \n",
    "                                    (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        \n",
    "                        cv2.imshow(\"Data Collection\", annotated_frame)\n",
    "                        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                    \n",
    "                    out.release()  # Finish saving the video\n",
    "                    \n",
    "                    # Save the sequence if it's complete\n",
    "                    if len(sequence) == sequence_length:\n",
    "                        sequence = np.array(sequence)\n",
    "                        filename = os.path.join(output_dir, f\"{gesture_label}_{sample_count+1}.npy\")\n",
    "                        np.save(filename, sequence)\n",
    "                        print(f\"Saved sample {sample_count+1} as {filename}\")\n",
    "                        print(f\"Saved raw video as {video_filename}\")\n",
    "                        sample_count += 1\n",
    "                        \n",
    "                        # Show success message with preview time\n",
    "                        success_start = time.time()\n",
    "                        while time.time() - success_start < 2:  # 2 second success message\n",
    "                            cv2.putText(annotated_frame, \"Sample recorded successfully!\", \n",
    "                                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                            cv2.imshow(\"Data Collection\", annotated_frame)\n",
    "                            cv2.waitKey(1)\n",
    "                    else:\n",
    "                        print(\"Incomplete sample, discarding...\")\n",
    "            else:\n",
    "                # Normal display mode (not recording)\n",
    "                cv2.putText(annotated_frame, f\"Gesture: {gesture_label} | Sample {sample_count+1}/{num_samples}\",\n",
    "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                cv2.putText(annotated_frame, \"Press 's' to start, 'q' to quit\", \n",
    "                            (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show the frame with annotations\n",
    "            cv2.imshow(\"Data Collection\", annotated_frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            \n",
    "            if key == ord('s') and not countdown_active:\n",
    "                # Start countdown before recording\n",
    "                countdown_active = True\n",
    "                countdown_timer = time.time() + 3  # 3 second countdown\n",
    "                print(\"Starting countdown...\")\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Data collection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_videos(\n",
    "    video_dir=\"../data/videos\",\n",
    "    output_dir=\"../data/raw\",\n",
    "    sequence_length=30,\n",
    "):\n",
    "    \"\"\"Extract features from videos with consistent dimensions\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith((\".avi\", \".mp4\"))]\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        print(f\"Processing {video_file}...\")\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        with mp_holistic.Holistic(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as holistic:\n",
    "            \n",
    "            sequence = []\n",
    "            previous_results_buffer = []\n",
    "            \n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(frame_rgb)\n",
    "                \n",
    "                # Always use extract_all_features for consistency\n",
    "                features = extract_all_features(results, previous_results_buffer)\n",
    "                sequence.append(features)\n",
    "                \n",
    "                # Update buffer\n",
    "                if len(previous_results_buffer) >= 5:\n",
    "                    previous_results_buffer.pop(0)\n",
    "                previous_results_buffer.append(results)\n",
    "                \n",
    "                # Check if we have enough frames\n",
    "                if len(sequence) == sequence_length:\n",
    "                    # Debug: Check all feature dimensions\n",
    "                    feature_lengths = [len(feat) for feat in sequence]\n",
    "                    print(f\"Feature dimensions: min={min(feature_lengths)}, max={max(feature_lengths)}\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Convert to numpy array - should work now with consistent dimensions\n",
    "                        sequence_array = np.array(sequence)\n",
    "                        filename = os.path.join(output_dir, f\"{video_file.split('.')[0]}.npy\")\n",
    "                        np.save(filename, sequence_array)\n",
    "                        print(f\"Saved sequence to {filename}, shape: {sequence_array.shape}\")\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error creating array: {e}\")\n",
    "                        # As fallback, save using object array\n",
    "                        sequence_array = np.array(sequence, dtype=object)\n",
    "                        filename = os.path.join(output_dir, f\"{video_file.split('.')[0]}_object.npy\")\n",
    "                        np.save(filename, sequence_array)\n",
    "                        print(f\"Saved as object array to {filename}\")\n",
    "                    \n",
    "                    sequence = []\n",
    "            \n",
    "            cap.release()\n",
    "            print(f\"Finished processing video: {video_file}\")\n",
    "    \n",
    "    print(\"All videos processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = \"../data\"\n",
    "RAW_DATA_DIR = os.path.join(BASE_DATA_DIR, \"raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DATA_DIR, \"processed\")\n",
    "VIDEO_DIR = os.path.join(BASE_DATA_DIR, \"videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_from_videos(\n",
    "    video_dir=VIDEO_DIR,\n",
    "    output_dir=RAW_DATA_DIR,\n",
    "    sequence_length=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
